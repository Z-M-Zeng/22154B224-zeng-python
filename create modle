import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

# 设置随机种子确保结果可复现
np.random.seed(42)
torch.manual_seed(42)

# 1. ARIMA模型实现
class ARIMAModel:
    def __init__(self, d=1):
        self.d = d
        self.model = None
    
    def fit(self, data, p=2, q=1):
        """训练ARIMA模型"""
        self.p = p
        self.q = q
        self.model = ARIMA(data, order=(p, self.d, q))
        self.results = self.model.fit()
        return self
    
    def predict(self, steps=1):
        """预测未来steps步"""
        return self.results.forecast(steps=steps)
    
    def evaluate(self, y_true, y_pred):
        """评估模型性能"""
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        return {"RMSE": rmse, "MAE": mae}
    
    @staticmethod
    def determine_order(data):
        """自动确定ARIMA模型阶数"""
        # ADF检验确定差分阶数d
        result = adfuller(data)
        d = 0 if result[1] < 0.05 else 1
        
        # 简化版：固定p=2, q=1，实际应用中可通过网格搜索优化
        return 2, d, 1


# 2. LSTM模型实现
class LSTMModel:
    def __init__(self, time_steps=30, units=64):
        self.time_steps = time_steps
        self.units = units
        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
    
    def prepare_data(self, data):
        """数据预处理：归一化和序列构建"""
        scaled_data = self.scaler.fit_transform(data.values.reshape(-1, 1))
        X, y = [], []
        for i in range(len(scaled_data) - self.time_steps):
            X.append(scaled_data[i:(i + self.time_steps), 0])
            y.append(scaled_data[i + self.time_steps, 0])
        return np.array(X), np.array(y), self.scaler
    
    def build_model(self):
        """构建LSTM模型"""
        model = Sequential()
        model.add(LSTM(units=self.units, return_sequences=False, 
                      input_shape=(self.time_steps, 1)))
        model.add(Dropout(0.2))
        model.add(Dense(units=1))
        model.compile(optimizer='adam', loss='mean_squared_error')
        self.model = model
        return self
    
    def fit(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):
        """训练LSTM模型"""
        early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
        self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping],
            verbose=0
        )
        return self
    
    def predict(self, X):
        """预测"""
        return self.model.predict(X)
    
    def inverse_transform(self, data):
        """反归一化"""
        return self.scaler.inverse_transform(data.reshape(-1, 1)).flatten()


# 3. ARIMA-LSTM混合模型
class ARIMALSTMModel:
    def __init__(self, arima_steps=30, lstm_steps=30):
        self.arima = ARIMAModel()
        self.lstm = LSTMModel(time_steps=lstm_steps)
        self.arima_steps = arima_steps
    
    def fit(self, data):
        """训练混合模型"""
        # 训练ARIMA模型
        p, d, q = self.arima.determine_order(data)
        self.arima.fit(data, p, q)
        
        # 计算ARIMA残差并训练LSTM
        arima_pred = self.arima.results.fittedvalues
        residuals = data - arima_pred
        X, y, self.lstm.scaler = self.lstm.prepare_data(residuals)
        
        # 划分训练集和验证集
        train_size = int(len(X) * 0.8)
        X_train, X_val = X[:train_size], X[train_size:]
        y_train, y_val = y[:train_size], y[train_size:]
        
        self.lstm.build_model()
        self.lstm.fit(X_train, y_train, X_val, y_val)
        return self
    
    def predict(self, data, steps=1):
        """混合模型预测"""
        # ARIMA预测线性部分
        arima_pred = self.arima.predict(steps=steps)
        
        # 准备LSTM输入（使用最近arima_steps个残差）
        arima_fitted = self.arima.results.fittedvalues
        residuals = data - arima_fitted
        X, _, _ = self.lstm.prepare_data(residuals)
        
        if len(X) >= self.lstm.time_steps:
            lstm_input = X[-self.lstm.time_steps:].reshape(1, self.lstm.time_steps, 1)
            lstm_pred = self.lstm.predict(lstm_input)
            lstm_pred = self.lstm.inverse_transform(lstm_pred)
        else:
            lstm_pred = np.zeros(steps)
        
        # 合并预测结果
        final_pred = arima_pred + lstm_pred
        return final_pred
    
    def evaluate(self, y_true, y_pred):
        """评估模型性能"""
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        return {"RMSE": rmse, "MAE": mae}


# 4. BERT情感分析模型
class BERTSentimentAnalyzer:
    def __init__(self, model_name="bert-base-chinese", num_labels=3):
        self.model_name = model_name
        self.num_labels = num_labels
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels
        )
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
    
    def preprocess_text(self, texts, max_length=128):
        """文本预处理：分词和向量化"""
        return self.tokenizer(
            texts,
            max_length=max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
    
    def finetune(self, train_texts, train_labels, val_texts, val_labels, epochs=5, batch_size=16):
        """微调BERT模型"""
        # 转换数据
        train_encodings = self.preprocess_text(train_texts)
        val_encodings = self.preprocess_text(val_texts)
        
        # 转换为Dataset
        class TextDataset(torch.utils.data.Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels
            
            def __getitem__(self, idx):
                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                item["labels"] = torch.tensor(self.labels[idx])
                return item
            
            def __len__(self):
                return len(self.labels)
        
        train_dataset = TextDataset(train_encodings, train_labels)
        val_dataset = TextDataset(val_encodings, val_labels)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)
        
        # 优化器
        optimizer = AdamW(self.model.parameters(), lr=2e-5)
        
        # 训练
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            for batch in train_loader:
                optimizer.zero_grad()
                batch = {k: v.to(self.device) for k, v in batch.items()}
                outputs = self.model(** batch)
                loss = outputs.loss
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            # 验证
            self.model.eval()
            val_acc = 0
            val_total = 0
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) for k, v in batch.items()}
                    outputs = self.model(** batch)
                    logits = outputs.logits
                    preds = torch.argmax(logits, dim=1)
                    val_acc += (preds == batch["labels"]).sum().item()
                    val_total += len(batch["labels"])
            
            print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Val Acc: {val_acc/val_total:.4f}")
        
        return self
    
    def predict(self, texts, batch_size=32):
        """预测情感"""
        self.model.eval()
        all_preds = []
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                encodings = self.preprocess_text(batch_texts)
                encodings = {k: v.to(self.device) for k, v in encodings.items()}
                outputs = self.model(** encodings)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                all_preds.extend(preds)
        return all_preds
    
    def calculate_sentiment_index(self, predictions):
        """计算情感指数"""
        # 假设0=负面, 1=中性, 2=正面
        sentiment_scores = np.zeros(len(predictions))
        sentiment_scores[predictions == 0] = -1  # 负面
        sentiment_scores[predictions == 2] = 1   # 正面
        return sentiment_scores.mean()


# 5. 投资组合优化
class PortfolioOptimizer:
    def __init__(self, risk_free_rate=0.03):
        self.risk_free_rate = risk_free_rate
    
    def calculate_returns(self, data):
        """计算日收益率"""
        returns = data.pct_change().dropna()
        return returns
    
    def mean_variance_optimization(self, returns, target_return=None):
        """均值-方差优化"""
        n = returns.shape[1]
        
        # 预期收益率
        mu = returns.mean()
        
        # 协方差矩阵
        cov = returns.cov()
        
        # 目标函数：最小化波动率
        def objective(weights):
            volatility = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))
            return volatility
        
        # 约束条件
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        
        # 权重边界
        bounds = [(0, 1) for _ in range(n)]
        
        if target_return is not None:
            # 带目标收益率的约束
            constraints.append({
                'type': 'eq', 
                'fun': lambda w: np.dot(mu, w) - target_return
            })
        
        # 初始权重
        initial_weights = np.ones(n) / n
        
        # 优化
        result = minimize(
            objective, 
            initial_weights, 
            method='SLSQP', 
            bounds=bounds, 
            constraints=constraints
        )
        
        return result.x
    
    def sharpe_ratio(self, returns, weights):
        """计算夏普比率"""
        portfolio_return = np.dot(returns.mean(), weights) * 252
        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(returns.cov(), weights))) * np.sqrt(252)
        sharpe = (portfolio_return - self.risk_free_rate) / portfolio_volatility
        return sharpe, portfolio_return, portfolio_volatility
    
    def optimize_for_sharpe(self, returns):
        """最大化夏普比率"""
        n = returns.shape[1]
        
        # 目标函数：负夏普比率（最小化负夏普相当于最大化夏普）
        def objective(weights):
            port_return = np.dot(returns.mean(), weights) * 252
            port_vol = np.sqrt(np.dot(weights.T, np.dot(returns.cov(), weights))) * np.sqrt(252)
            sharpe = (port_return - self.risk_free_rate) / port_vol if port_vol > 0 else -np.inf
            return -sharpe
        
        # 约束条件
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]
        bounds = [(0, 1) for _ in range(n)]
        initial_weights = np.ones(n) / n
        
        result = minimize(
            objective, 
            initial_weights, 
            method='SLSQP', 
            bounds=bounds, 
            constraints=constraints
        )
        
        return result.x


# 6. 完整建模与分析流程
def stock_prediction_pipeline():
    """股票预测完整流程"""
    # ============== 1. 加载数据 ==============
    print("加载股票数据...")
    stock_data = pd.read_excel("stock_dataset.xlsx", sheet_name=None)
    
    # 选择贵州茅台数据示例
    guizhou_moutai = stock_data["600519_贵州茅台"]
    guizhou_moutai.set_index("日期", inplace=True)
    close_prices = guizhou_moutai["收盘价"]
    
    # ============== 2. ARIMA模型 ==============
    print("\n训练ARIMA模型...")
    arima = ARIMAModel()
    p, d, q = arima.determine_order(close_prices)
    arima.fit(close_prices, p, q)
    arima_pred = arima.results.fittedvalues
    
    arima_rmse = np.sqrt(mean_squared_error(close_prices[p+d:], arima_pred[p+d:]))
    print(f"ARIMA模型 RMSE: {arima_rmse:.2f}")
    
    # ============== 3. LSTM模型 ==============
    print("\n训练LSTM模型...")
    lstm = LSTMModel(time_steps=60)
    X, y, scaler = lstm.prepare_data(close_prices)
    
    # 划分训练集和测试集
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    lstm.build_model()
    lstm.fit(X_train, y_train, X_test, y_test)
    
    lstm_pred_scaled = lstm.predict(X_test)
    lstm_pred = lstm.inverse_transform(lstm_pred_scaled)
    lstm_rmse = np.sqrt(mean_squared_error(close_prices[train_size+lstm.time_steps:], lstm_pred))
    print(f"LSTM模型 RMSE: {lstm_rmse:.2f}")
    
    # ============== 4. ARIMA-LSTM混合模型 ==============
    print("\n训练ARIMA-LSTM混合模型...")
    hybrid = ARIMALSTMModel(arima_steps=p+d, lstm_steps=60)
    hybrid.fit(close_prices)
    
    # 预测测试集
    hybrid_pred = hybrid.predict(close_prices, steps=len(X_test))
    hybrid_rmse = np.sqrt(mean_squared_error(close_prices[train_size+lstm.time_steps:], hybrid_pred))
    print(f"ARIMA-LSTM混合模型 RMSE: {hybrid_rmse:.2f}")
    
    # 可视化预测结果
    plt.figure(figsize=(12, 6))
    test_dates = close_prices.index[train_size+lstm.time_steps:]
    plt.plot(test_dates, close_prices[train_size+lstm.time_steps:], label="实际价格")
    plt.plot(test_dates, hybrid_pred, label="混合模型预测", alpha=0.7)
    plt.title("贵州茅台股价预测结果")
    plt.xlabel("日期")
    plt.ylabel("收盘价")
    plt.legend()
    plt.grid(True)
    plt.savefig("stock_prediction.png")
    plt.show()
    
    # ============== 5. 情感分析（示例） ==============
    print("\n加载新闻数据进行情感分析...")
    # 实际应用中应加载真实新闻数据
    sample_news = [
        "贵州茅台一季度净利润同比增长15%，超出市场预期",
        "白酒行业面临政策调整风险，短期股价承压",
        "机构调研显示茅台终端需求依然强劲",
        "市场担忧消费疲软，白酒板块集体下跌",
        "茅台推出新产品线，拓展年轻消费群体"
    ]
    
    # 假设情感标签（0=负面, 1=中性, 2=正面）
    sample_labels = [2, 0, 2, 0, 2]
    
    print("训练BERT情感分析模型...")
    sentiment_analyzer = BERTSentimentAnalyzer()
    sentiment_analyzer.finetune(sample_news, sample_labels, sample_news, sample_labels, epochs=2)
    
    print("分析新闻情感...")
    predictions = sentiment_analyzer.predict(sample_news)
    sentiment_index = sentiment_analyzer.calculate_sentiment_index(predictions)
    print(f"新闻情感指数: {sentiment_index:.2f} (正值表示正面情绪)")
    
    # ============== 6. 投资组合优化 ==============
    print("\n投资组合优化...")
    # 假设多只股票数据
    stock_names = ["600519_贵州茅台", "00700_腾讯控股", "002594_比亚迪", "300750_宁德时代", "601398_工商银行"]
    multi_stock_data = {name: stock_data[name]["收盘价"] for name in stock_names}
    multi_stock_df = pd.DataFrame(multi_stock_data)
    
    # 计算收益率
    returns = multi_stock_df.pct_change().dropna()
    
    # 优化器
    optimizer = PortfolioOptimizer(risk_free_rate=0.03)
    
    # 最大化夏普比率
    optimal_weights = optimizer.optimize_for_sharpe(returns)
    
    # 计算夏普比率
    sharpe, port_return, port_vol = optimizer.sharpe_ratio(returns, optimal_weights)
    
    print("最优投资组合权重:")
    for i, name in enumerate(stock_names):
        print(f"{name}: {optimal_weights[i]:.4f}")
    print(f"投资组合年化收益率: {port_return:.4f}")
    print(f"投资组合年化波动率: {port_vol:.4f}")
    print(f"夏普比率: {sharpe:.4f}")
    
    return {
        "arima_rmse": arima_rmse,
        "lstm_rmse": lstm_rmse,
        "hybrid_rmse": hybrid_rmse,
        "sentiment_index": sentiment_index,
        "portfolio_sharpe": sharpe
    }


# 运行完整流程
if __name__ == "__main__":
    results = stock_prediction_pipeline()
    print("\n=== 模型性能总结 ===")
    for key, value in results.items():
        print(f"{key}: {value:.4f}")
